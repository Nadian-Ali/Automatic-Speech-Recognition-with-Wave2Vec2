{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aba891e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.12\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5607ea8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.8.4\n",
      "  latest version: 4.11.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/ec2-user/anaconda3/envs/s2t\n",
      "\n",
      "  added / updated specs:\n",
      "    - ipywidgets\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    argon2-cffi-bindings-21.2.0|   py37h5e8e339_1          34 KB  conda-forge\n",
      "    importlib-metadata-4.11.1  |   py37h89c1867_0          32 KB  conda-forge\n",
      "    jsonschema-4.4.0           |     pyhd8ed1ab_0          57 KB  conda-forge\n",
      "    markupsafe-2.1.0           |   py37h540881e_0          22 KB  conda-forge\n",
      "    nbclient-0.5.11            |     pyhd8ed1ab_0          64 KB  conda-forge\n",
      "    nbconvert-6.4.2            |   py37h89c1867_0         532 KB  conda-forge\n",
      "    notebook-6.4.8             |     pyha770c72_0         6.3 MB  conda-forge\n",
      "    pandoc-2.17.1.1            |       ha770c72_0        11.9 MB  conda-forge\n",
      "    prometheus_client-0.13.1   |     pyhd8ed1ab_0          47 KB  conda-forge\n",
      "    pyparsing-3.0.7            |     pyhd8ed1ab_0          79 KB  conda-forge\n",
      "    pyrsistent-0.18.1          |   py37h5e8e339_0          90 KB  conda-forge\n",
      "    terminado-0.13.1           |   py37h89c1867_0          27 KB  conda-forge\n",
      "    typing_extensions-4.1.1    |     pyha770c72_0          29 KB  conda-forge\n",
      "    zipp-3.7.0                 |     pyhd8ed1ab_1          12 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        19.2 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  argon2-cffi        conda-forge/noarch::argon2-cffi-21.3.0-pyhd8ed1ab_0\n",
      "  argon2-cffi-bindi~ conda-forge/linux-64::argon2-cffi-bindings-21.2.0-py37h5e8e339_1\n",
      "  attrs              conda-forge/noarch::attrs-21.4.0-pyhd8ed1ab_0\n",
      "  bleach             conda-forge/noarch::bleach-4.1.0-pyhd8ed1ab_0\n",
      "  cffi               conda-forge/linux-64::cffi-1.15.0-py37h036bc23_0\n",
      "  defusedxml         conda-forge/noarch::defusedxml-0.7.1-pyhd8ed1ab_0\n",
      "  flit-core          conda-forge/noarch::flit-core-3.6.0-pyhd8ed1ab_0\n",
      "  importlib-metadata conda-forge/linux-64::importlib-metadata-4.11.1-py37h89c1867_0\n",
      "  importlib_resourc~ conda-forge/noarch::importlib_resources-5.4.0-pyhd8ed1ab_0\n",
      "  ipython_genutils   conda-forge/noarch::ipython_genutils-0.2.0-py_1\n",
      "  ipywidgets         conda-forge/noarch::ipywidgets-7.6.5-pyhd8ed1ab_0\n",
      "  jinja2             conda-forge/noarch::jinja2-3.0.3-pyhd8ed1ab_0\n",
      "  jsonschema         conda-forge/noarch::jsonschema-4.4.0-pyhd8ed1ab_0\n",
      "  jupyterlab_pygmen~ conda-forge/noarch::jupyterlab_pygments-0.1.2-pyh9f0ad1d_0\n",
      "  jupyterlab_widgets conda-forge/noarch::jupyterlab_widgets-1.0.2-pyhd8ed1ab_0\n",
      "  markupsafe         conda-forge/linux-64::markupsafe-2.1.0-py37h540881e_0\n",
      "  mistune            conda-forge/linux-64::mistune-0.8.4-py37h5e8e339_1005\n",
      "  nbclient           conda-forge/noarch::nbclient-0.5.11-pyhd8ed1ab_0\n",
      "  nbconvert          conda-forge/linux-64::nbconvert-6.4.2-py37h89c1867_0\n",
      "  nbformat           conda-forge/noarch::nbformat-5.1.3-pyhd8ed1ab_0\n",
      "  notebook           conda-forge/noarch::notebook-6.4.8-pyha770c72_0\n",
      "  packaging          conda-forge/noarch::packaging-21.3-pyhd8ed1ab_0\n",
      "  pandoc             conda-forge/linux-64::pandoc-2.17.1.1-ha770c72_0\n",
      "  pandocfilters      conda-forge/noarch::pandocfilters-1.5.0-pyhd8ed1ab_0\n",
      "  prometheus_client  conda-forge/noarch::prometheus_client-0.13.1-pyhd8ed1ab_0\n",
      "  pycparser          conda-forge/noarch::pycparser-2.21-pyhd8ed1ab_0\n",
      "  pyparsing          conda-forge/noarch::pyparsing-3.0.7-pyhd8ed1ab_0\n",
      "  pyrsistent         conda-forge/linux-64::pyrsistent-0.18.1-py37h5e8e339_0\n",
      "  send2trash         conda-forge/noarch::send2trash-1.8.0-pyhd8ed1ab_0\n",
      "  terminado          conda-forge/linux-64::terminado-0.13.1-py37h89c1867_0\n",
      "  testpath           conda-forge/noarch::testpath-0.5.0-pyhd8ed1ab_0\n",
      "  typing_extensions  conda-forge/noarch::typing_extensions-4.1.1-pyha770c72_0\n",
      "  webencodings       conda-forge/noarch::webencodings-0.5.1-py_1\n",
      "  widgetsnbextension conda-forge/linux-64::widgetsnbextension-3.5.2-py37h89c1867_1\n",
      "  zipp               conda-forge/noarch::zipp-3.7.0-pyhd8ed1ab_1\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "nbclient-0.5.11      | 64 KB     | ##################################### | 100% \n",
      "pyparsing-3.0.7      | 79 KB     | ##################################### | 100% \n",
      "terminado-0.13.1     | 27 KB     | ##################################### | 100% \n",
      "jsonschema-4.4.0     | 57 KB     | ##################################### | 100% \n",
      "prometheus_client-0. | 47 KB     | ##################################### | 100% \n",
      "typing_extensions-4. | 29 KB     | ##################################### | 100% \n",
      "pandoc-2.17.1.1      | 11.9 MB   | ##################################### | 100% \n",
      "importlib-metadata-4 | 32 KB     | ##################################### | 100% \n",
      "notebook-6.4.8       | 6.3 MB    | ##################################### | 100% \n",
      "pyrsistent-0.18.1    | 90 KB     | ##################################### | 100% \n",
      "markupsafe-2.1.0     | 22 KB     | ##################################### | 100% \n",
      "nbconvert-6.4.2      | 532 KB    | ##################################### | 100% \n",
      "zipp-3.7.0           | 12 KB     | ##################################### | 100% \n",
      "argon2-cffi-bindings | 34 KB     | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: - Enabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: \u001b[32mOK\u001b[0m\n",
      "\n",
      "done\n",
      "Enabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: \u001b[32mOK\u001b[0m\n",
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.8.4\n",
      "  latest version: 4.11.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/ec2-user/anaconda3/envs/s2t\n",
      "\n",
      "  added / updated specs:\n",
      "    - cudatoolkit=10.2\n",
      "    - pytorch\n",
      "    - torchaudio\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    blas-1.0                   |              mkl           6 KB\n",
      "    cudatoolkit-10.2.89        |      h8f6ccaa_10       449.9 MB  conda-forge\n",
      "    libuv-1.43.0               |       h7f98852_0         1.0 MB  conda-forge\n",
      "    mkl-service-2.4.0          |   py37h402132d_0          60 KB  conda-forge\n",
      "    mkl_fft-1.3.1              |   py37h3e078e5_1         206 KB  conda-forge\n",
      "    numpy-1.21.2               |   py37h20f2e39_0          23 KB\n",
      "    numpy-base-1.21.2          |   py37h79a1101_0         4.8 MB\n",
      "    pytorch-1.10.2             |py3.7_cuda10.2_cudnn7.6.5_0       768.3 MB  pytorch\n",
      "    pytorch-mutex-1.0          |             cuda           3 KB  pytorch\n",
      "    torchaudio-0.10.2          |       py37_cu102         4.5 MB  pytorch\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        1.20 GB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  blas               pkgs/main/linux-64::blas-1.0-mkl\n",
      "  cudatoolkit        conda-forge/linux-64::cudatoolkit-10.2.89-h8f6ccaa_10\n",
      "  intel-openmp       pkgs/main/linux-64::intel-openmp-2021.4.0-h06a4308_3561\n",
      "  libuv              conda-forge/linux-64::libuv-1.43.0-h7f98852_0\n",
      "  mkl                pkgs/main/linux-64::mkl-2021.4.0-h06a4308_640\n",
      "  mkl-service        conda-forge/linux-64::mkl-service-2.4.0-py37h402132d_0\n",
      "  mkl_fft            conda-forge/linux-64::mkl_fft-1.3.1-py37h3e078e5_1\n",
      "  mkl_random         conda-forge/linux-64::mkl_random-1.2.2-py37h219a48f_0\n",
      "  numpy              pkgs/main/linux-64::numpy-1.21.2-py37h20f2e39_0\n",
      "  numpy-base         pkgs/main/linux-64::numpy-base-1.21.2-py37h79a1101_0\n",
      "  pytorch            pytorch/linux-64::pytorch-1.10.2-py3.7_cuda10.2_cudnn7.6.5_0\n",
      "  pytorch-mutex      pytorch/noarch::pytorch-mutex-1.0-cuda\n",
      "  torchaudio         pytorch/linux-64::torchaudio-0.10.2-py37_cu102\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "mkl_fft-1.3.1        | 206 KB    | ##################################### | 100% \n",
      "torchaudio-0.10.2    | 4.5 MB    | ##################################### | 100% \n",
      "cudatoolkit-10.2.89  | 449.9 MB  | ##################################### | 100% \n",
      "libuv-1.43.0         | 1.0 MB    | ##################################### | 100% \n",
      "numpy-base-1.21.2    | 4.8 MB    | ##################################### | 100% \n",
      "pytorch-1.10.2       | 768.3 MB  | ##################################### | 100% \n",
      "blas-1.0             | 6 KB      | ##################################### | 100% \n",
      "pytorch-mutex-1.0    | 3 KB      | ##################################### | 100% \n",
      "numpy-1.21.2         | 23 KB     | ##################################### | 100% \n",
      "mkl-service-2.4.0    | 60 KB     | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: / By downloading and using the CUDA Toolkit conda packages, you accept the terms and conditions of the CUDA End User License Agreement (EULA): https://docs.nvidia.com/cuda/eula/index.html\n",
      "\n",
      "done\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting datasets==1.13.3\n",
      "  Downloading datasets-1.13.3-py3-none-any.whl (287 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.1/287.1 KB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting aiohttp\n",
      "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata in /home/ec2-user/anaconda3/envs/s2t/lib/python3.7/site-packages (from datasets==1.13.3) (4.11.1)\n",
      "Collecting fsspec[http]>=2021.05.0\n",
      "  Downloading fsspec-2022.1.0-py3-none-any.whl (133 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.2/133.2 KB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting huggingface-hub<0.1.0,>=0.0.19\n",
      "  Downloading huggingface_hub-0.0.19-py3-none-any.whl (56 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 KB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pandas\n",
      "  Downloading pandas-1.3.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m102.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting multiprocess\n",
      "  Using cached multiprocess-0.70.12.2-py37-none-any.whl (112 kB)\n",
      "Collecting dill\n",
      "  Using cached dill-0.3.4-py2.py3-none-any.whl (86 kB)\n",
      "Collecting tqdm>=4.62.1\n",
      "  Downloading tqdm-4.62.3-py2.py3-none-any.whl (76 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.2/76.2 KB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting requests>=2.19.0\n",
      "  Using cached requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
      "Collecting pyarrow!=4.0.0,>=1.0.0\n",
      "  Downloading pyarrow-7.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/s2t/lib/python3.7/site-packages (from datasets==1.13.3) (1.21.2)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.9/243.9 KB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/s2t/lib/python3.7/site-packages (from datasets==1.13.3) (21.3)\n",
      "Collecting pyyaml\n",
      "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m596.3/596.3 KB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting filelock\n",
      "  Downloading filelock-3.6.0-py3-none-any.whl (10.0 kB)\n",
      "Requirement already satisfied: typing-extensions in /home/ec2-user/anaconda3/envs/s2t/lib/python3.7/site-packages (from huggingface-hub<0.1.0,>=0.0.19->datasets==1.13.3) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/s2t/lib/python3.7/site-packages (from packaging->datasets==1.13.3) (3.0.7)\n",
      "Collecting idna<4,>=2.5\n",
      "  Using cached idna-3.3-py3-none-any.whl (61 kB)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Using cached urllib3-1.26.8-py2.py3-none-any.whl (138 kB)\n",
      "Collecting charset-normalizer~=2.0.0\n",
      "  Downloading charset_normalizer-2.0.12-py3-none-any.whl (39 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Using cached certifi-2021.10.8-py2.py3-none-any.whl (149 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 KB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/s2t/lib/python3.7/site-packages (from aiohttp->datasets==1.13.3) (21.4.0)\n",
      "Collecting asynctest==0.13.0\n",
      "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.8/144.8 KB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m271.8/271.8 KB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/s2t/lib/python3.7/site-packages (from importlib-metadata->datasets==1.13.3) (3.7.0)\n",
      "Collecting pytz>=2017.3\n",
      "  Using cached pytz-2021.3-py2.py3-none-any.whl (503 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/ec2-user/anaconda3/envs/s2t/lib/python3.7/site-packages (from pandas->datasets==1.13.3) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/s2t/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets==1.13.3) (1.16.0)\n",
      "Installing collected packages: pytz, certifi, xxhash, urllib3, tqdm, pyyaml, pyarrow, multidict, idna, fsspec, frozenlist, filelock, dill, charset-normalizer, asynctest, async-timeout, yarl, requests, pandas, multiprocess, aiosignal, huggingface-hub, aiohttp, datasets\n",
      "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 certifi-2021.10.8 charset-normalizer-2.0.12 datasets-1.13.3 dill-0.3.4 filelock-3.6.0 frozenlist-1.3.0 fsspec-2022.1.0 huggingface-hub-0.0.19 idna-3.3 multidict-6.0.2 multiprocess-0.70.12.2 pandas-1.3.5 pyarrow-7.0.0 pytz-2021.3 pyyaml-6.0 requests-2.27.1 tqdm-4.62.3 urllib3-1.26.8 xxhash-2.0.2 yarl-1.7.2\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting transformers==4.11.3\n",
      "  Downloading transformers-4.11.3-py3-none-any.whl (2.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.0.17 in /home/ec2-user/anaconda3/envs/s2t/lib/python3.7/site-packages (from transformers==4.11.3) (0.0.19)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m895.2/895.2 KB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/s2t/lib/python3.7/site-packages (from transformers==4.11.3) (1.21.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/s2t/lib/python3.7/site-packages (from transformers==4.11.3) (21.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/s2t/lib/python3.7/site-packages (from transformers==4.11.3) (4.62.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/s2t/lib/python3.7/site-packages (from transformers==4.11.3) (6.0)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/s2t/lib/python3.7/site-packages (from transformers==4.11.3) (2.27.1)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2022.1.18-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (748 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m749.0/749.0 KB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/s2t/lib/python3.7/site-packages (from transformers==4.11.3) (3.6.0)\n",
      "Requirement already satisfied: importlib-metadata in /home/ec2-user/anaconda3/envs/s2t/lib/python3.7/site-packages (from transformers==4.11.3) (4.11.1)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /home/ec2-user/anaconda3/envs/s2t/lib/python3.7/site-packages (from huggingface-hub>=0.0.17->transformers==4.11.3) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/s2t/lib/python3.7/site-packages (from packaging>=20.0->transformers==4.11.3) (3.0.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/s2t/lib/python3.7/site-packages (from importlib-metadata->transformers==4.11.3) (3.7.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/s2t/lib/python3.7/site-packages (from requests->transformers==4.11.3) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/s2t/lib/python3.7/site-packages (from requests->transformers==4.11.3) (1.26.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ec2-user/anaconda3/envs/s2t/lib/python3.7/site-packages (from requests->transformers==4.11.3) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/s2t/lib/python3.7/site-packages (from requests->transformers==4.11.3) (2021.10.8)\n",
      "Collecting joblib\n",
      "  Downloading joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.0/307.0 KB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting click\n",
      "  Downloading click-8.0.4-py3-none-any.whl (97 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.5/97.5 KB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six in /home/ec2-user/anaconda3/envs/s2t/lib/python3.7/site-packages (from sacremoses->transformers==4.11.3) (1.16.0)\n",
      "Installing collected packages: tokenizers, regex, joblib, click, sacremoses, transformers\n",
      "Successfully installed click-8.0.4 joblib-1.1.0 regex-2022.1.18 sacremoses-0.0.47 tokenizers-0.10.3 transformers-4.11.3\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting librosa\n",
      "  Downloading librosa-0.9.1-py3-none-any.whl (213 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.1/213.1 KB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting scipy>=1.2.0\n",
      "  Downloading scipy-1.7.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.1/38.1 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting soundfile>=0.10.2\n",
      "  Downloading SoundFile-0.10.3.post1-py2.py3-none-any.whl (21 kB)\n",
      "Collecting pooch>=1.0\n",
      "  Downloading pooch-1.6.0-py3-none-any.whl (56 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 KB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: joblib>=0.14 in /home/ec2-user/anaconda3/envs/s2t/lib/python3.7/site-packages (from librosa) (1.1.0)\n",
      "Collecting resampy>=0.2.2\n",
      "  Downloading resampy-0.2.2.tar.gz (323 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.4/323.4 KB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/s2t/lib/python3.7/site-packages (from librosa) (21.3)\n",
      "Collecting scikit-learn>=0.19.1\n",
      "  Downloading scikit_learn-1.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (24.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.8/24.8 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting audioread>=2.1.5\n",
      "  Downloading audioread-2.1.9.tar.gz (377 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.5/377.5 KB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17.0 in /home/ec2-user/anaconda3/envs/s2t/lib/python3.7/site-packages (from librosa) (1.21.2)\n",
      "Collecting numba>=0.45.1\n",
      "  Downloading numba-0.55.1-1-cp37-cp37m-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: decorator>=4.0.10 in /home/ec2-user/anaconda3/envs/s2t/lib/python3.7/site-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/s2t/lib/python3.7/site-packages (from numba>=0.45.1->librosa) (60.9.3)\n",
      "Collecting llvmlite<0.39,>=0.38.0rc1\n",
      "  Downloading llvmlite-0.38.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.5/34.5 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/s2t/lib/python3.7/site-packages (from packaging>=20.0->librosa) (3.0.7)\n",
      "Collecting appdirs>=1.3.0\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/ec2-user/anaconda3/envs/s2t/lib/python3.7/site-packages (from pooch>=1.0->librosa) (2.27.1)\n",
      "Requirement already satisfied: six>=1.3 in /home/ec2-user/anaconda3/envs/s2t/lib/python3.7/site-packages (from resampy>=0.2.2->librosa) (1.16.0)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: cffi>=1.0 in /home/ec2-user/anaconda3/envs/s2t/lib/python3.7/site-packages (from soundfile>=0.10.2->librosa) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /home/ec2-user/anaconda3/envs/s2t/lib/python3.7/site-packages (from cffi>=1.0->soundfile>=0.10.2->librosa) (2.21)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/s2t/lib/python3.7/site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (1.26.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ec2-user/anaconda3/envs/s2t/lib/python3.7/site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/s2t/lib/python3.7/site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/s2t/lib/python3.7/site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.3)\n",
      "Building wheels for collected packages: audioread, resampy\n",
      "  Building wheel for audioread (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for audioread: filename=audioread-2.1.9-py3-none-any.whl size=23153 sha256=06a761d510010008f5dcb82fef731de61de4133e55a90ca89153285010a61118\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/ba/7b/eb/213741ccc0678f63e346ab8dff10495995ca3f426af87b8d88\n",
      "  Building wheel for resampy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for resampy: filename=resampy-0.2.2-py3-none-any.whl size=320732 sha256=6e14525c9161b337161250d946e05be6b61066368ed81791e8ec2f2b9d2fc021\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/a0/18/0a/8ad18a597d8333a142c9789338a96a6208f1198d290ece356c\n",
      "Successfully built audioread resampy\n",
      "Installing collected packages: audioread, appdirs, threadpoolctl, scipy, llvmlite, soundfile, scikit-learn, pooch, numba, resampy, librosa\n",
      "Successfully installed appdirs-1.4.4 audioread-2.1.9 librosa-0.9.1 llvmlite-0.38.0 numba-0.55.1 pooch-1.6.0 resampy-0.2.2 scikit-learn-1.0.2 scipy-1.7.3 soundfile-0.10.3.post1 threadpoolctl-3.1.0\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting jiwer\n",
      "  Downloading jiwer-2.3.0-py3-none-any.whl (15 kB)\n",
      "Collecting python-Levenshtein==0.12.2\n",
      "  Downloading python-Levenshtein-0.12.2.tar.gz (50 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 KB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/s2t/lib/python3.7/site-packages (from python-Levenshtein==0.12.2->jiwer) (60.9.3)\n",
      "Building wheels for collected packages: python-Levenshtein\n",
      "  Building wheel for python-Levenshtein (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for python-Levenshtein: filename=python_Levenshtein-0.12.2-cp37-cp37m-linux_x86_64.whl size=167459 sha256=33296b18c6a929eda6d9a36b13383c8400c7e4960f42eff1cceaf6e36933ef7b\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/05/5f/ca/7c4367734892581bb5ff896f15027a932c551080b2abd3e00d\n",
      "Successfully built python-Levenshtein\n",
      "Installing collected packages: python-Levenshtein, jiwer\n",
      "Successfully installed jiwer-2.3.0 python-Levenshtein-0.12.2\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting s3fs\n",
      "  Downloading s3fs-2022.1.0-py3-none-any.whl (25 kB)\n",
      "Collecting aiobotocore~=2.1.0\n",
      "  Downloading aiobotocore-2.1.1.tar.gz (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 KB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: aiohttp<=4 in /home/ec2-user/anaconda3/envs/s2t/lib/python3.7/site-packages (from s3fs) (3.8.1)\n",
      "Requirement already satisfied: fsspec==2022.01.0 in /home/ec2-user/anaconda3/envs/s2t/lib/python3.7/site-packages (from s3fs) (2022.1.0)\n",
      "Collecting botocore<1.23.25,>=1.23.24\n",
      "  Downloading botocore-1.23.24-py3-none-any.whl (8.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting wrapt>=1.10.10\n",
      "  Downloading wrapt-1.13.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (79 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.4/79.4 KB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aioitertools>=0.5.1\n",
      "  Downloading aioitertools-0.9.0-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/s2t/lib/python3.7/site-packages (from aiohttp<=4->s3fs) (21.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ec2-user/anaconda3/envs/s2t/lib/python3.7/site-packages (from aiohttp<=4->s3fs) (1.3.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/s2t/lib/python3.7/site-packages (from aiohttp<=4->s3fs) (6.0.2)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /home/ec2-user/anaconda3/envs/s2t/lib/python3.7/site-packages (from aiohttp<=4->s3fs) (2.0.12)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ec2-user/anaconda3/envs/s2t/lib/python3.7/site-packages (from aiohttp<=4->s3fs) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /home/ec2-user/anaconda3/envs/s2t/lib/python3.7/site-packages (from aiohttp<=4->s3fs) (4.1.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/ec2-user/anaconda3/envs/s2t/lib/python3.7/site-packages (from aiohttp<=4->s3fs) (4.0.2)\n",
      "Requirement already satisfied: asynctest==0.13.0 in /home/ec2-user/anaconda3/envs/s2t/lib/python3.7/site-packages (from aiohttp<=4->s3fs) (0.13.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ec2-user/anaconda3/envs/s2t/lib/python3.7/site-packages (from aiohttp<=4->s3fs) (1.7.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/s2t/lib/python3.7/site-packages (from botocore<1.23.25,>=1.23.24->aiobotocore~=2.1.0->s3fs) (2.8.2)\n",
      "Collecting jmespath<1.0.0,>=0.7.1\n",
      "  Using cached jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/s2t/lib/python3.7/site-packages (from botocore<1.23.25,>=1.23.24->aiobotocore~=2.1.0->s3fs) (1.26.8)\n",
      "Requirement already satisfied: idna>=2.0 in /home/ec2-user/anaconda3/envs/s2t/lib/python3.7/site-packages (from yarl<2.0,>=1.0->aiohttp<=4->s3fs) (3.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/s2t/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.23.25,>=1.23.24->aiobotocore~=2.1.0->s3fs) (1.16.0)\n",
      "Building wheels for collected packages: aiobotocore\n",
      "  Building wheel for aiobotocore (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for aiobotocore: filename=aiobotocore-2.1.1-py3-none-any.whl size=55483 sha256=c3bd8a696dd583d7ed79c40b55a30c0da15bfa2890e934675d99db94f7a56dbe\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/52/fd/d3/e85dd95a98c12426ed6864dc24a408c3ca3253d4387b2f1cce\n",
      "Successfully built aiobotocore\n",
      "Installing collected packages: wrapt, jmespath, aioitertools, botocore, aiobotocore, s3fs\n",
      "Successfully installed aiobotocore-2.1.1 aioitertools-0.9.0 botocore-1.23.24 jmespath-0.10.0 s3fs-2022.1.0 wrapt-1.13.3\n"
     ]
    }
   ],
   "source": [
    "#%%capture\n",
    "\n",
    "#enable the process bar\n",
    "!conda install -y -c conda-forge ipywidgets\n",
    "!jupyter nbextension enable --py widgetsnbextension\n",
    "\n",
    "!conda install -y pytorch torchaudio cudatoolkit=10.2 -c pytorch\n",
    "!pip install      datasets==1.13.3\n",
    "!pip install      transformers==4.11.3\n",
    "!pip install      librosa\n",
    "!pip install      jiwer\n",
    "!pip install      s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1eccf6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "!pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fdf087cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7bb5a62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_session = boto3.session.Session()\n",
    "s3 = my_session.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fccd7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert AWS keys here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2bfdbe42",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource(\n",
    "service_name='s3',\n",
    "region_name='eu-west-1',\n",
    "aws_access_key_id= data['aws_access_key_id'],\n",
    "aws_secret_access_key= data['aws_secret_access_key'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a197347f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset common_voice (/home/ec2-user/SageMaker/dataset/common_voice/tr/6.1.0/f7a9d973839b7706e9e281c19b7e512f31badf3c0fdbd21c671f3c4bf9acf3b9)\n",
      "Reusing dataset common_voice (/home/ec2-user/SageMaker/dataset/common_voice/tr/6.1.0/f7a9d973839b7706e9e281c19b7e512f31badf3c0fdbd21c671f3c4bf9acf3b9)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_metric, Audio\n",
    "\n",
    "common_voice_train = load_dataset(\"common_voice\", \"tr\", split=\"train+validation\",cache_dir='/home/ec2-user/SageMaker/dataset')\n",
    "common_voice_test = load_dataset(\"common_voice\", \"tr\", split=\"test\",cache_dir='/home/ec2-user/SageMaker/dataset/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d12f732",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_voice_train = common_voice_train.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"segment\", \"up_votes\"])\n",
    "common_voice_test = common_voice_test.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"segment\", \"up_votes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de4ab46d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['path', 'audio', 'sentence'],\n",
       "    num_rows: 3478\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_voice_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f7f11b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "chars_to_ignore_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"\\“\\%\\‘\\”\\�]'\n",
    "\n",
    "def remove_special_characters(batch):\n",
    "    batch[\"sentence\"] = re.sub(chars_to_ignore_regex, '', batch[\"sentence\"]).lower() + \" \"\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8cbe2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "903d4169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4f503c20625430eb675d7c4564243f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3478 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ec2-user/SageMaker/dataset/common_voice/tr/6.1.0/f7a9d973839b7706e9e281c19b7e512f31badf3c0fdbd21c671f3c4bf9acf3b9/cache-1782cb846a879a9f.arrow\n"
     ]
    }
   ],
   "source": [
    "common_voice_train = common_voice_train.map(remove_special_characters)\n",
    "common_voice_test = common_voice_test.map(remove_special_characters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62456b10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['path', 'audio', 'sentence'],\n",
       "    num_rows: 1647\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_voice_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfbbb141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbcc2cd9cecb450da70a6c7fd6d47b25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3478 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ec2-user/SageMaker/dataset/common_voice/tr/6.1.0/f7a9d973839b7706e9e281c19b7e512f31badf3c0fdbd21c671f3c4bf9acf3b9/cache-e747932f924194e0.arrow\n"
     ]
    }
   ],
   "source": [
    "def replace_hatted_characters(batch):\n",
    "    batch[\"sentence\"] = re.sub('[â]', 'a', batch[\"sentence\"])\n",
    "    batch[\"sentence\"] = re.sub('[î]', 'i', batch[\"sentence\"])\n",
    "    batch[\"sentence\"] = re.sub('[ô]', 'o', batch[\"sentence\"])\n",
    "    batch[\"sentence\"] = re.sub('[û]', 'u', batch[\"sentence\"])\n",
    "    return batch\n",
    "\n",
    "common_voice_train = common_voice_train.map(replace_hatted_characters)\n",
    "common_voice_test = common_voice_test.map(replace_hatted_characters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aba7ce87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['path', 'audio', 'sentence'],\n",
       "    num_rows: 3478\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_voice_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "888c32c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_chars(batch):\n",
    "  all_text = \" \".join(batch[\"sentence\"])\n",
    "  vocab = list(set(all_text))\n",
    "  return {\"vocab\": [vocab], \"all_text\": [all_text]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f43c7682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7f8f8e44bce4a47ae6685917912ab5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f62d3cb6ad484db3970ca0f4c70574a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vocab_train = common_voice_train.map(extract_all_chars, batched=True, batch_size=-1, keep_in_memory=True, remove_columns=common_voice_train.column_names)\n",
    "vocab_test = common_voice_test.map(extract_all_chars, batched=True, batch_size=-1, keep_in_memory=True, remove_columns=common_voice_test.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d91d48b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['path', 'audio', 'sentence'],\n",
       "    num_rows: 3478\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_voice_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3097276",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list = list(set(vocab_train[\"vocab\"][0]) | set(vocab_test[\"vocab\"][0]))\n",
    "vocab_dict = {v: k for k, v in enumerate(vocab_list)}\n",
    "vocab_dict[\"|\"] = vocab_dict[\" \"]\n",
    "del vocab_dict[\" \"]\n",
    "vocab_dict[\"[UNK]\"] = len(vocab_dict)\n",
    "vocab_dict[\"[PAD]\"] = len(vocab_dict)\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f74c43b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '6HNAAPQB4K1D381P',\n",
       "  'HostId': 'KHEOQupY1PiMo27h8WPQTOVwW4SQVsWMxK+fNxuXn1U1u1V6ShQtmfvY20/04tMWGfcG38twd6c=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'KHEOQupY1PiMo27h8WPQTOVwW4SQVsWMxK+fNxuXn1U1u1V6ShQtmfvY20/04tMWGfcG38twd6c=',\n",
       "   'x-amz-request-id': '6HNAAPQB4K1D381P',\n",
       "   'date': 'Mon, 21 Feb 2022 11:25:57 GMT',\n",
       "   'etag': '\"fa5fc16d6b1c7d6f9f026e9a65eb4127\"',\n",
       "   'server': 'AmazonS3',\n",
       "   'content-length': '0'},\n",
       "  'RetryAttempts': 0},\n",
       " 'ETag': '\"fa5fc16d6b1c7d6f9f026e9a65eb4127\"'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('vocab.json', 'w') as vocab_file:\n",
    "    json.dump(vocab_dict, vocab_file)\n",
    "s3.Object('commonvoicesdataset', 'Turkish/vocab.json').put(Body=json.dumps(vocab_dict,indent = 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e47cca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this tokenizer is using the previous json file/ so becareful what u load\n",
    "from transformers import Wav2Vec2CTCTokenizer\n",
    "tokenizer = Wav2Vec2CTCTokenizer(\"./vocab.json\", unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "249d4805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/ec2-user/SageMaker/tokenizer_config.json',\n",
       " '/home/ec2-user/SageMaker/special_tokens_map.json',\n",
       " '/home/ec2-user/SageMaker/vocab.json',\n",
       " '/home/ec2-user/SageMaker/added_tokens.json')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained('/home/ec2-user/SageMaker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96348922",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2FeatureExtractor\n",
    "feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11829771",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2Processor\n",
    "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3210b380",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ec2-user/SageMaker/STT'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e96616ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_voice_train = common_voice_train.cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "common_voice_test  = common_voice_test.cast_column(\"audio\", Audio(sampling_rate=16_000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26252169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['path', 'audio', 'sentence'],\n",
       "    num_rows: 3478\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_voice_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "34aae1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # batched output is \"un-batched\"\n",
    "    batch[\"input_values\"] = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_values[0]\n",
    "    \n",
    "    with processor.as_target_processor():\n",
    "        batch[\"labels\"] = processor(batch[\"sentence\"]).input_ids\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eaf8d50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_voice_train = common_voice_train.map(prepare_dataset, remove_columns=common_voice_train.column_names, num_proc=4)\n",
    "common_voice_test = common_voice_test.map(prepare_dataset, remove_columns=common_voice_test.column_names, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "358c6690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_values', 'labels'],\n",
       "    num_rows: 3478\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_voice_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5b123bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data ={'aws_access_key_id' : 'AKIASS57IE7SLCJ2F4XJ','aws_secret_access_key' : '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',  \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "62cf17f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_access_key_id=data['aws_access_key_id']\n",
    "aws_secret_access_key=data['aws_secret_access_key']\n",
    "s3  = boto3.resource('s3',aws_access_key_id=aws_access_key_id,aws_secret_access_key=aws_secret_access_key)\n",
    "s3c = boto3.client('s3',aws_access_key_id=aws_access_key_id,aws_secret_access_key=aws_secret_access_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1a843f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#from datasets.filesystems import S3FileSystem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "389e2777",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting s3fs\n",
      "  Downloading s3fs-2022.1.0-py3-none-any.whl (25 kB)\n",
      "Collecting aiobotocore~=2.1.0\n",
      "  Downloading aiobotocore-2.1.1.tar.gz (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 KB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: fsspec==2022.01.0 in /home/ec2-user/anaconda3/envs/stt/lib/python3.7/site-packages (from s3fs) (2022.1.0)\n",
      "Requirement already satisfied: aiohttp<=4 in /home/ec2-user/anaconda3/envs/stt/lib/python3.7/site-packages (from s3fs) (3.8.1)\n",
      "Collecting botocore<1.23.25,>=1.23.24\n",
      "  Downloading botocore-1.23.24-py3-none-any.whl (8.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting wrapt>=1.10.10\n",
      "  Downloading wrapt-1.13.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (79 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.4/79.4 KB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aioitertools>=0.5.1\n",
      "  Downloading aioitertools-0.9.0-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ec2-user/anaconda3/envs/stt/lib/python3.7/site-packages (from aiohttp<=4->s3fs) (1.3.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/ec2-user/anaconda3/envs/stt/lib/python3.7/site-packages (from aiohttp<=4->s3fs) (4.0.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/stt/lib/python3.7/site-packages (from aiohttp<=4->s3fs) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /home/ec2-user/anaconda3/envs/stt/lib/python3.7/site-packages (from aiohttp<=4->s3fs) (4.1.1)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /home/ec2-user/anaconda3/envs/stt/lib/python3.7/site-packages (from aiohttp<=4->s3fs) (2.0.12)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ec2-user/anaconda3/envs/stt/lib/python3.7/site-packages (from aiohttp<=4->s3fs) (1.7.2)\n",
      "Requirement already satisfied: asynctest==0.13.0 in /home/ec2-user/anaconda3/envs/stt/lib/python3.7/site-packages (from aiohttp<=4->s3fs) (0.13.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/stt/lib/python3.7/site-packages (from aiohttp<=4->s3fs) (21.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ec2-user/anaconda3/envs/stt/lib/python3.7/site-packages (from aiohttp<=4->s3fs) (1.2.0)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/stt/lib/python3.7/site-packages (from botocore<1.23.25,>=1.23.24->aiobotocore~=2.1.0->s3fs) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/stt/lib/python3.7/site-packages (from botocore<1.23.25,>=1.23.24->aiobotocore~=2.1.0->s3fs) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/stt/lib/python3.7/site-packages (from botocore<1.23.25,>=1.23.24->aiobotocore~=2.1.0->s3fs) (1.26.8)\n",
      "Requirement already satisfied: idna>=2.0 in /home/ec2-user/anaconda3/envs/stt/lib/python3.7/site-packages (from yarl<2.0,>=1.0->aiohttp<=4->s3fs) (3.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/stt/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.23.25,>=1.23.24->aiobotocore~=2.1.0->s3fs) (1.16.0)\n",
      "Building wheels for collected packages: aiobotocore\n",
      "  Building wheel for aiobotocore (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for aiobotocore: filename=aiobotocore-2.1.1-py3-none-any.whl size=55483 sha256=bba75f387baccedfbd6e1f48c2e9a5a438ea3a2588aa7bbca3cfa2c78c46c080\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/52/fd/d3/e85dd95a98c12426ed6864dc24a408c3ca3253d4387b2f1cce\n",
      "Successfully built aiobotocore\n",
      "Installing collected packages: wrapt, aioitertools, botocore, aiobotocore, s3fs\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.24.0\n",
      "    Uninstalling botocore-1.24.0:\n",
      "      Successfully uninstalled botocore-1.24.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "boto3 1.21.0 requires botocore<1.25.0,>=1.24.0, but you have botocore 1.23.24 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed aiobotocore-2.1.1 aioitertools-0.9.0 botocore-1.23.24 s3fs-2022.1.0 wrapt-1.13.3\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c4c41555",
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "69f4027c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "key = data['aws_access_key_id']\n",
    "secret = data['aws_secret_access_key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1d861e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3fsystems = s3fs.S3FileSystem(key=key, secret=secret)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a2d9c421",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_voice_train.save_to_disk('s3://commonvoicesdataset/Turkish/train_set',fs=s3fsystems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "610d543a",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_voice_test.save_to_disk('s3://commonvoicesdataset/Turkish/test_set',fs=s3fsystems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "22f1a7f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_values', 'labels'],\n",
       "    num_rows: 1647\n",
       "})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_voice_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38ec4bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f33ba34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "48d65af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = load_from_disk('s3://commonvoicesdataset/turkish/test',fs=s3fsystems)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a42996b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = load_from_disk('s3://commonvoicesdataset/turkish/train',fs=s3fsystems) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16a74d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset1 = load_from_disk('s3://commonvoicesdataset/test',fs=s31)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e6ee2664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# from dataclasses import dataclass, field\n",
    "# from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "# @dataclass\n",
    "# class DataCollatorCTCWithPadding:\n",
    "#     \"\"\"\n",
    "#     Data collator that will dynamically pad the inputs received.\n",
    "#     Args:\n",
    "#         processor (:class:`~transformers.Wav2Vec2Processor`)\n",
    "#             The processor used for proccessing the data.\n",
    "#         padding (:obj:`bool`, :obj:`str` or :class:`a~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "#             Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "#             among:\n",
    "#             * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "#               sequence if provided).\n",
    "#             * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "#               maximum acceptable input length for the model if that argument is not provided.\n",
    "#             * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "#               different lengths).\n",
    "#         max_length (:obj:`int`, `optional`):\n",
    "#             Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n",
    "#         max_length_labels (:obj:`int`, `optional`):\n",
    "#             Maximum length of the ``labels`` returned list and optionally padding length (see above).\n",
    "#         pad_to_multiple_of (:obj:`int`, `optional`):\n",
    "#             If set will pad the sequence to a multiple of the provided value.\n",
    "#             This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
    "#             7.5 (Volta).\n",
    "#     \"\"\"\n",
    "\n",
    "#     processor: Wav2Vec2Processor\n",
    "#     padding: Union[bool, str] = True\n",
    "#     max_length: Optional[int] = None\n",
    "#     max_length_labels: Optional[int] = None\n",
    "#     pad_to_multiple_of: Optional[int] = None\n",
    "#     pad_to_multiple_of_labels: Optional[int] = None\n",
    "\n",
    "#     def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "#         # split inputs and labels since they have to be of different lenghts and need\n",
    "#         # different padding methods\n",
    "#         input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "#         label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "#         batch = self.processor.pad(\n",
    "#             input_features,\n",
    "#             padding=self.padding,\n",
    "#             max_length=self.max_length,\n",
    "#             pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "#             return_tensors=\"pt\",\n",
    "#         )\n",
    "#         with self.processor.as_target_processor():\n",
    "#             labels_batch = self.processor.pad(\n",
    "#                 label_features,\n",
    "#                 padding=self.padding,\n",
    "#                 max_length=self.max_length_labels,\n",
    "#                 pad_to_multiple_of=self.pad_to_multiple_of_labels,\n",
    "#                 return_tensors=\"pt\",\n",
    "#             )\n",
    "\n",
    "#         # replace padding with -100 to ignore loss correctly\n",
    "#         labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "#         batch[\"labels\"] = labels\n",
    "\n",
    "#         return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "adee92a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8230eca0f7b14d749acf4bf8bd652c39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.95k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import numpy  as np\n",
    "# data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)\n",
    "# wer_metric = load_metric(\"wer\")\n",
    "\n",
    "# def compute_metrics(pred):\n",
    "#     pred_logits = pred.predictions\n",
    "#     pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "#     pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "#     pred_str = processor.batch_decode(pred_ids)\n",
    "#     # we do not want to group tokens when computing the metrics\n",
    "#     label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "\n",
    "#     wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "#     return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "26d87d66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_values', 'labels'],\n",
       "    num_rows: 1647\n",
       "})"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "637016f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eb85be038a644e0b0a74023cbd0c10f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.53k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "452a65ef5d0d4e0b8f05e240767c502f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.18G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-xls-r-300m were not used when initializing Wav2Vec2ForCTC: ['project_q.weight', 'quantizer.weight_proj.bias', 'quantizer.codevectors', 'project_q.bias', 'quantizer.weight_proj.weight', 'project_hid.weight', 'project_hid.bias']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-xls-r-300m and are newly initialized: ['lm_head.bias', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# from transformers import Wav2Vec2ForCTC\n",
    "\n",
    "# model = Wav2Vec2ForCTC.from_pretrained(\n",
    "#     \"facebook/wav2vec2-xls-r-300m\", #facebook/wav2vec2-xls-r-300m\n",
    "#     attention_dropout=0.1,\n",
    "#     hidden_dropout=0.1,\n",
    "#     feat_proj_dropout=0.0,\n",
    "#     mask_time_prob=0.05,\n",
    "#     layerdrop=0.1,\n",
    "#     ctc_loss_reduction=\"mean\", \n",
    "#     pad_token_id=processor.tokenizer.pad_token_id,\n",
    "#     vocab_size=len(processor.tokenizer)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "75af9a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.freeze_feature_extractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9cb3b6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import TrainingArguments\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#   # output_dir=\"/content/gdrive/MyDrive/wav2vec2-large-xlsr-turkish-demo\",\n",
    "#   output_dir=\"./Turkish\",\n",
    "#   group_by_length=True,\n",
    "#   per_device_train_batch_size=2,\n",
    "#   gradient_accumulation_steps=2,\n",
    "#   evaluation_strategy=\"steps\", \n",
    "#   gradient_checkpointing=True,\n",
    "#   num_train_epochs=1,\n",
    "#   fp16=True,\n",
    "#   save_steps=400,\n",
    "#   eval_steps=400,\n",
    "#   logging_steps=400,\n",
    "#   learning_rate=3e-4,\n",
    "#   warmup_steps=500,\n",
    "#   save_total_limit=2,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3a75b4e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp fp16 backend\n"
     ]
    }
   ],
   "source": [
    "# from transformers import Trainer\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     data_collator=data_collator,\n",
    "#     args=training_args,\n",
    "#     compute_metrics=compute_metrics,\n",
    "#     train_dataset=train,\n",
    "#     eval_dataset=test,\n",
    "#     tokenizer=processor.feature_extractor,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f5c8d3cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Feb 16 11:54:42 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.142.00   Driver Version: 450.142.00   CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            On   | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   47C    P0    26W /  70W |   5455MiB / 15109MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      6214      C   ...onda3/envs/stt/bin/python     2123MiB |\n",
      "|    0   N/A  N/A     11364      C   ...onda3/envs/stt/bin/python     3329MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "61687d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !sudo kill -9 3288                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a27ba111",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 3478\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 869\n",
      "/home/ec2-user/anaconda3/envs/s2t/lib/python3.7/site-packages/transformers/feature_extraction_utils.py:158: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1640811756765/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  tensor = as_tensor(value)\n",
      "/home/ec2-user/anaconda3/envs/s2t/lib/python3.7/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:882: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  return (input_length - kernel_size) // stride + 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='143' max='869' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [143/869 01:34 < 08:06, 1.49 it/s, Epoch 0.16/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_16526/4032920361.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/s2t/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1314\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1316\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m                 if (\n",
      "\u001b[0;32m~/anaconda3/envs/s2t/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   1845\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_amp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/s2t/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   1879\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1880\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1881\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1882\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1883\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/s2t/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/s2t/lib/python3.7/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_values, attention_mask, output_attentions, output_hidden_states, return_dict, labels)\u001b[0m\n\u001b[1;32m   1497\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1498\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1499\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1500\u001b[0m         )\n\u001b[1;32m   1501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/s2t/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/s2t/lib/python3.7/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_values, attention_mask, mask_time_indices, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1079\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1081\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1082\u001b[0m         )\n\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/s2t/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/s2t/lib/python3.7/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    729\u001b[0m                         \u001b[0mcreate_custom_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m                         \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 731\u001b[0;31m                         \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    732\u001b[0m                     )\n\u001b[1;32m    733\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/s2t/lib/python3.7/site-packages/torch/utils/checkpoint.py\u001b[0m in \u001b[0;36mcheckpoint\u001b[0;34m(function, *args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unexpected keyword arguments: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\",\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mCheckpointFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreserve\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/s2t/lib/python3.7/site-packages/torch/utils/checkpoint.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, run_function, preserve_rng_state, *args)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/s2t/lib/python3.7/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py\u001b[0m in \u001b[0;36mcustom_forward\u001b[0;34m(*inputs)\u001b[0m\n\u001b[1;32m    722\u001b[0m                     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_custom_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m                         \u001b[0;32mdef\u001b[0m \u001b[0mcustom_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 724\u001b[0;31m                             \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    725\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m                         \u001b[0;32mreturn\u001b[0m \u001b[0mcustom_forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/s2t/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/s2t/lib/python3.7/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m         hidden_states, attn_weights, _ = self.attention(\n\u001b[0;32m--> 571\u001b[0;31m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m         )\n\u001b[1;32m    573\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/s2t/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/s2t/lib/python3.7/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m             \u001b[0;31m# self_attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m             \u001b[0mkey_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbsz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m             \u001b[0mvalue_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbsz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/s2t/lib/python3.7/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py\u001b[0m in \u001b[0;36m_shape\u001b[0;34m(self, tensor, seq_len, bsz)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbsz\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m     def forward(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8353e2eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ec2-user/SageMaker/STT'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d77aabc8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload failed: Turkish/checkpoint-400/preprocessor_config.json to s3://bucketname/checkpoint-400/preprocessor_config.json An error occurred (AccessDenied) when calling the PutObject operation: Access Denied\n",
      "upload failed: Turkish/checkpoint-400/scaler.pt to s3://bucketname/checkpoint-400/scaler.pt An error occurred (AccessDenied) when calling the PutObject operation: Access Denied\n",
      "upload failed: Turkish/checkpoint-400/trainer_state.json to s3://bucketname/checkpoint-400/trainer_state.json An error occurred (AccessDenied) when calling the PutObject operation: Access Denied\n",
      "upload failed: Turkish/checkpoint-400/rng_state.pth to s3://bucketname/checkpoint-400/rng_state.pth An error occurred (AccessDenied) when calling the PutObject operation: Access Denied\n",
      "upload failed: Turkish/checkpoint-400/scheduler.pt to s3://bucketname/checkpoint-400/scheduler.pt An error occurred (AccessDenied) when calling the PutObject operation: Access Denied\n",
      "upload failed: Turkish/checkpoint-400/config.json to s3://bucketname/checkpoint-400/config.json An error occurred (AccessDenied) when calling the PutObject operation: Access Denied\n",
      "upload failed: Turkish/checkpoint-400/training_args.bin to s3://bucketname/checkpoint-400/training_args.bin An error occurred (AccessDenied) when calling the PutObject operation: Access Denied\n",
      "upload failed: Turkish/checkpoint-800/scheduler.pt to s3://bucketname/checkpoint-800/scheduler.pt An error occurred (AccessDenied) when calling the PutObject operation: Access Denied\n",
      "upload failed: Turkish/checkpoint-800/config.json to s3://bucketname/checkpoint-800/config.json An error occurred (AccessDenied) when calling the PutObject operation: Access Denied\n",
      "upload failed: Turkish/checkpoint-800/scaler.pt to s3://bucketname/checkpoint-800/scaler.pt An error occurred (AccessDenied) when calling the PutObject operation: Access Denied\n",
      "upload failed: Turkish/checkpoint-800/preprocessor_config.json to s3://bucketname/checkpoint-800/preprocessor_config.json An error occurred (AccessDenied) when calling the PutObject operation: Access Denied\n",
      "upload failed: Turkish/checkpoint-800/rng_state.pth to s3://bucketname/checkpoint-800/rng_state.pth An error occurred (AccessDenied) when calling the PutObject operation: Access Denied\n",
      "upload failed: Turkish/checkpoint-400/pytorch_model.bin to s3://bucketname/checkpoint-400/pytorch_model.bin An error occurred (AccessDenied) when calling the CreateMultipartUpload operation: Access Denied\n",
      "upload failed: Turkish/checkpoint-800/pytorch_model.bin to s3://bucketname/checkpoint-800/pytorch_model.bin An error occurred (AccessDenied) when calling the CreateMultipartUpload operation: Access Denied\n",
      "upload failed: Turkish/checkpoint-400/optimizer.pt to s3://bucketname/checkpoint-400/optimizer.pt An error occurred (AccessDenied) when calling the CreateMultipartUpload operation: Access Denied\n",
      "upload failed: Turkish/checkpoint-800/optimizer.pt to s3://bucketname/checkpoint-800/optimizer.pt An error occurred (AccessDenied) when calling the CreateMultipartUpload operation: Access Denied\n",
      "upload failed: Turkish/checkpoint-800/trainer_state.json to s3://bucketname/checkpoint-800/trainer_state.json An error occurred (AccessDenied) when calling the PutObject operation: Access Denied\n",
      "upload failed: Turkish/checkpoint-800/training_args.bin to s3://bucketname/checkpoint-800/training_args.bin An error occurred (AccessDenied) when calling the PutObject operation: Access Denied\n"
     ]
    }
   ],
   "source": [
    "# !aws s3 cp --recursive ./Turkish/ s3://commonvoicesdataset/Turkish//"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_s2t",
   "language": "python",
   "name": "conda_s2t"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
